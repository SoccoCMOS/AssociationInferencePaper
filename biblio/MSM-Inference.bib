Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Dillon2017,
abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
archivePrefix = {arXiv},
arxivId = {1711.10604},
author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
eprint = {1711.10604},
file = {:C$\backslash$:/Users/simoussi/Thesis/My{\_}Research{\_}questions/Biblio/1711.10604.pdf:pdf},
keywords = {27,a variational,auto-encoder,deep learning,figure 1,general pattern for training,probabilistic programming,probability distributions,transformations,vae},
title = {{TensorFlow Distributions}},
url = {http://arxiv.org/abs/1711.10604},
year = {2017}
}
@article{Tran2017,
abstract = {Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform existing genetics methods by an absolute difference of 15-45.3{\%}.},
archivePrefix = {arXiv},
arxivId = {1710.10742},
author = {Tran, Dustin and Blei, David M.},
eprint = {1710.10742},
file = {:C$\backslash$:/Users/simoussi/Thesis/My{\_}Research{\_}questions/Biblio/1710.10742.pdf:pdf},
pages = {1--18},
title = {{Implicit Causal Models for Genome-wide Association Studies}},
url = {http://arxiv.org/abs/1710.10742},
year = {2017}
}
